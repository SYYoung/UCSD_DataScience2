{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14)\n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./sentiment-logistic-regression/sentiment_labelled_sentences/full_set.txt\",\n",
    "         encoding='utf-8') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preprocessing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = [str(x) for x in range(10)]\n",
    "\n",
    "# remove digits\n",
    "digit_less = [full_remove(x, digits) for x in sentences]\n",
    "# remove punctation\n",
    "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
    "# make everything lowercase\n",
    "sents_lower = [x.lower() for x in punc_less]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so there is no way for me to plug it in here in the us unless i go by a converter ',\n",
       " 'good case  excellent value ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_lower[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./stopwords\") as f:\n",
    "    stop_word = f.readlines()\n",
    "stop_word = [x.strip() for x in stop_word]  \n",
    "len(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['way plug us unless go converter',\n",
       " 'good case excellent value',\n",
       " 'great jawbone',\n",
       " 'tied charger conversations lasting minutes major problems',\n",
       " 'mic great',\n",
       " 'jiggle plug line right decent volume',\n",
       " 'several dozen several hundred contacts imagine fun sending one one',\n",
       " 'razr owner must',\n",
       " 'needless say wasted money',\n",
       " 'waste money time']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_split = [x.split() for x in sents_lower]\n",
    "sents_processed =[\" \".join(list(filter(lambda a: a not in stop_word, x))) for x in sents_split]\n",
    "sents_processed[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 4501)\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None,\n",
    "                            max_features=4500)\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "p_mat = data_features.toarray()\n",
    "data_mat = np.ones((p_mat.shape[0],p_mat.shape[1]+1))\n",
    "data_mat[:,:-1] = p_mat\n",
    "print(data_mat.shape)\n",
    "print(data_mat[0:3, 4499:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (2500, 4501)\n",
      "test data: (500, 4501)\n"
     ]
    }
   ],
   "source": [
    "# Training/test split\n",
    "np.random.seed(0)\n",
    "test_size = 500\n",
    "each_test_size = int(test_size/2)\n",
    "\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0],250,replace=False), \n",
    "                    np.random.choice((np.where(y==1))[0],250,replace=False))\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "\n",
    "train_data = data_mat[train_inds,]\n",
    "train_labels = y[train_inds]\n",
    "\n",
    "test_data = data_mat[test_inds,]\n",
    "test_labels = y[test_inds]\n",
    "\n",
    "print(\"train data:\", train_data.shape)\n",
    "print(\"test data:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error:  0.0096\n",
      "Testing error:  0.192\n"
     ]
    }
   ],
   "source": [
    "## fit a logistic regression model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"none\")\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "w = clf.coef_[0,:]\n",
    "b = clf.intercept_\n",
    "\n",
    "preds_train = clf.predict(train_data)\n",
    "preds_test = clf.predict(test_data)\n",
    "\n",
    "errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n",
    "\n",
    "print(\"Training error: \", float(errs_train)/len(train_labels))\n",
    "print(\"Testing error: \", float(errs_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
