{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14)\n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./sentiment-logistic-regression/sentiment_labelled_sentences/full_set.txt\",\n",
    "         encoding='utf-8') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preprocessing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = [str(x) for x in range(10)]\n",
    "\n",
    "# remove digits\n",
    "digit_less = [full_remove(x, digits) for x in sentences]\n",
    "# remove punctation\n",
    "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
    "# make everything lowercase\n",
    "sents_lower = [x.lower() for x in punc_less]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so there is no way for me to plug it in here in the us unless i go by a converter ',\n",
       " 'good case  excellent value ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_lower[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./stopwords\") as f:\n",
    "    stop_word = f.readlines()\n",
    "stop_word = [x.strip() for x in stop_word]  \n",
    "len(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['way plug us unless go converter',\n",
       " 'good case excellent value',\n",
       " 'great jawbone',\n",
       " 'tied charger conversations lasting minutes major problems',\n",
       " 'mic great',\n",
       " 'jiggle plug line right decent volume',\n",
       " 'several dozen several hundred contacts imagine fun sending one one',\n",
       " 'razr owner must',\n",
       " 'needless say wasted money',\n",
       " 'waste money time']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_split = [x.split() for x in sents_lower]\n",
    "sents_processed =[\" \".join(list(filter(lambda a: a not in stop_word, x))) for x in sents_split]\n",
    "sents_processed[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 4501)\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None,\n",
    "                            max_features=4500)\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "p_mat = data_features.toarray()\n",
    "data_mat = np.ones((p_mat.shape[0],p_mat.shape[1]+1))\n",
    "data_mat[:,:-1] = p_mat\n",
    "print(data_mat.shape)\n",
    "print(data_mat[0:3, 4499:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/test split\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
